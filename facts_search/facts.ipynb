{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search\n",
    "\n",
    "To create search I firstly tried the suggested approach, then tried Iterative PCA, then doc2vec and then a BERT model.\n",
    "\n",
    "The suggested approach could only work on the subset of the dataset, because of the memory limitations. \n",
    "\n",
    "Iterative PCA allowed the model to run, however the training process takes an hour to fit the pca model.\n",
    "\n",
    "doc2vec produced bad results for searching.\n",
    "\n",
    "BERT model produced pretty mediocre results and takes quite a long time to encode vectors.\n",
    "\n",
    "In every apprach I took a subset of samples either due to memory limitations or due to slow processing speed.\n",
    "\n",
    "As a last resort, I just took PCA, fitted it on a random subset of samples and transformed the whole dataset. It produced good results and was not running OOM, however it may not have selected best PC, that would explain best variance on the whole set.\n",
    "\n",
    "NOTE: It might be better to restart kernel and run first three cells before each approach due to memory limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>verifiable</th>\n",
       "      <th>label</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75397</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>Nikolaj Coster-Waldau worked with the Fox Broa...</td>\n",
       "      <td>[[[92206, 104971, Nikolaj_Coster-Waldau, 7], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150448</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>Roman Atwood is a content creator.</td>\n",
       "      <td>[[[174271, 187498, Roman_Atwood, 1]], [[174271...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214861</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>History of art includes architecture, dance, s...</td>\n",
       "      <td>[[[255136, 254645, History_of_art, 2]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156709</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>Adrienne Bailon is an accountant.</td>\n",
       "      <td>[[[180804, 193183, Adrienne_Bailon, 0]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83235</td>\n",
       "      <td>NOT VERIFIABLE</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>System of a Down briefly disbanded in limbo.</td>\n",
       "      <td>[[[100277, None, None, None]]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      verifiable            label  \\\n",
       "0   75397      VERIFIABLE         SUPPORTS   \n",
       "1  150448      VERIFIABLE         SUPPORTS   \n",
       "2  214861      VERIFIABLE         SUPPORTS   \n",
       "3  156709      VERIFIABLE          REFUTES   \n",
       "4   83235  NOT VERIFIABLE  NOT ENOUGH INFO   \n",
       "\n",
       "                                               claim  \\\n",
       "0  Nikolaj Coster-Waldau worked with the Fox Broa...   \n",
       "1                 Roman Atwood is a content creator.   \n",
       "2  History of art includes architecture, dance, s...   \n",
       "3                  Adrienne Bailon is an accountant.   \n",
       "4       System of a Down briefly disbanded in limbo.   \n",
       "\n",
       "                                            evidence  \n",
       "0  [[[92206, 104971, Nikolaj_Coster-Waldau, 7], [...  \n",
       "1  [[[174271, 187498, Roman_Atwood, 1]], [[174271...  \n",
       "2            [[[255136, 254645, History_of_art, 2]]]  \n",
       "3           [[[180804, 193183, Adrienne_Bailon, 0]]]  \n",
       "4                     [[[100277, None, None, None]]]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "ds_path = \"datasets/train.jsonl\"\n",
    "df = pd.read_json(ds_path, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>verifiable</th>\n",
       "      <th>label</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75397</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>Nikolaj Coster-Waldau worked with the Fox Broa...</td>\n",
       "      <td>[[[92206, 104971, Nikolaj_Coster-Waldau, 7], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150448</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>Roman Atwood is a content creator.</td>\n",
       "      <td>[[[174271, 187498, Roman_Atwood, 1]], [[174271...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214861</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>History of art includes architecture, dance, s...</td>\n",
       "      <td>[[[255136, 254645, History_of_art, 2]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>129629</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>Homeland is an American television spy thrille...</td>\n",
       "      <td>[[[151831, 166598, Homeland_-LRB-TV_series-RRB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33078</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>The Boston Celtics play their home games at TD...</td>\n",
       "      <td>[[[49158, 58489, Boston_Celtics, 3]], [[49159,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  verifiable     label  \\\n",
       "0   75397  VERIFIABLE  SUPPORTS   \n",
       "1  150448  VERIFIABLE  SUPPORTS   \n",
       "2  214861  VERIFIABLE  SUPPORTS   \n",
       "5  129629  VERIFIABLE  SUPPORTS   \n",
       "8   33078  VERIFIABLE  SUPPORTS   \n",
       "\n",
       "                                               claim  \\\n",
       "0  Nikolaj Coster-Waldau worked with the Fox Broa...   \n",
       "1                 Roman Atwood is a content creator.   \n",
       "2  History of art includes architecture, dance, s...   \n",
       "5  Homeland is an American television spy thrille...   \n",
       "8  The Boston Celtics play their home games at TD...   \n",
       "\n",
       "                                            evidence  \n",
       "0  [[[92206, 104971, Nikolaj_Coster-Waldau, 7], [...  \n",
       "1  [[[174271, 187498, Roman_Atwood, 1]], [[174271...  \n",
       "2            [[[255136, 254645, History_of_art, 2]]]  \n",
       "5  [[[151831, 166598, Homeland_-LRB-TV_series-RRB...  \n",
       "8  [[[49158, 58489, Boston_Celtics, 3]], [[49159,...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verif_df = df[(df['verifiable']=='VERIFIABLE') & (df['label']=='SUPPORTS')]\n",
    "verif_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(tokenizer=<function word_tokenize at 0x7f342ac05700>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "to_tdm = CountVectorizer(tokenizer=nltk.word_tokenize)\n",
    "to_tdm.fit(verif_df['claim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA+TDM+hsnw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = to_tdm.transform(verif_df['claim'])\n",
    "TDM = pd.DataFrame(X.toarray(), columns=to_tdm.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_transforming PCA. I took only 20000 samples because my RAM doesn't allow for more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8267685654458785\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1500)\n",
    "tdm_pca = pd.DataFrame(pca.fit_transform(TDM[:20000]))\n",
    "print(sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1490</th>\n",
       "      <th>1491</th>\n",
       "      <th>1492</th>\n",
       "      <th>1493</th>\n",
       "      <th>1494</th>\n",
       "      <th>1495</th>\n",
       "      <th>1496</th>\n",
       "      <th>1497</th>\n",
       "      <th>1498</th>\n",
       "      <th>1499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.274933</td>\n",
       "      <td>-0.403122</td>\n",
       "      <td>-0.190818</td>\n",
       "      <td>-0.426525</td>\n",
       "      <td>-0.058092</td>\n",
       "      <td>-0.038775</td>\n",
       "      <td>-0.581888</td>\n",
       "      <td>-0.146260</td>\n",
       "      <td>0.037089</td>\n",
       "      <td>-0.039497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030712</td>\n",
       "      <td>-0.003086</td>\n",
       "      <td>-0.059155</td>\n",
       "      <td>-0.000657</td>\n",
       "      <td>-0.011793</td>\n",
       "      <td>0.038035</td>\n",
       "      <td>0.014311</td>\n",
       "      <td>-0.028168</td>\n",
       "      <td>0.021815</td>\n",
       "      <td>-0.038385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.627415</td>\n",
       "      <td>0.324823</td>\n",
       "      <td>0.822102</td>\n",
       "      <td>0.093970</td>\n",
       "      <td>0.075213</td>\n",
       "      <td>0.061310</td>\n",
       "      <td>0.001915</td>\n",
       "      <td>-0.184241</td>\n",
       "      <td>0.143812</td>\n",
       "      <td>-0.045845</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028753</td>\n",
       "      <td>0.066659</td>\n",
       "      <td>0.047388</td>\n",
       "      <td>-0.090217</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.019931</td>\n",
       "      <td>0.030875</td>\n",
       "      <td>-0.018938</td>\n",
       "      <td>-0.024505</td>\n",
       "      <td>0.054116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.168995</td>\n",
       "      <td>6.258847</td>\n",
       "      <td>0.938537</td>\n",
       "      <td>-0.300605</td>\n",
       "      <td>-0.230533</td>\n",
       "      <td>-0.954852</td>\n",
       "      <td>0.466747</td>\n",
       "      <td>-0.259475</td>\n",
       "      <td>-0.463401</td>\n",
       "      <td>-0.286365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>-0.030375</td>\n",
       "      <td>-0.038482</td>\n",
       "      <td>-0.039791</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>-0.017464</td>\n",
       "      <td>-0.033838</td>\n",
       "      <td>0.035390</td>\n",
       "      <td>-0.080861</td>\n",
       "      <td>0.048241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.399398</td>\n",
       "      <td>-0.650302</td>\n",
       "      <td>0.701173</td>\n",
       "      <td>-0.791744</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.149518</td>\n",
       "      <td>0.746968</td>\n",
       "      <td>0.231181</td>\n",
       "      <td>-0.740373</td>\n",
       "      <td>-0.375252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027823</td>\n",
       "      <td>-0.038555</td>\n",
       "      <td>0.016616</td>\n",
       "      <td>-0.065417</td>\n",
       "      <td>0.053753</td>\n",
       "      <td>-0.037554</td>\n",
       "      <td>-0.006230</td>\n",
       "      <td>0.032986</td>\n",
       "      <td>0.054645</td>\n",
       "      <td>0.027326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.302666</td>\n",
       "      <td>-0.425505</td>\n",
       "      <td>-0.191070</td>\n",
       "      <td>-0.414490</td>\n",
       "      <td>-0.031847</td>\n",
       "      <td>-0.008057</td>\n",
       "      <td>-0.537349</td>\n",
       "      <td>-0.144615</td>\n",
       "      <td>0.060322</td>\n",
       "      <td>-0.075051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034048</td>\n",
       "      <td>0.019152</td>\n",
       "      <td>0.009938</td>\n",
       "      <td>0.023361</td>\n",
       "      <td>-0.008969</td>\n",
       "      <td>0.014082</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>-0.064524</td>\n",
       "      <td>0.039825</td>\n",
       "      <td>0.011461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.274933 -0.403122 -0.190818 -0.426525 -0.058092 -0.038775 -0.581888   \n",
       "1 -0.627415  0.324823  0.822102  0.093970  0.075213  0.061310  0.001915   \n",
       "2  6.168995  6.258847  0.938537 -0.300605 -0.230533 -0.954852  0.466747   \n",
       "3  0.399398 -0.650302  0.701173 -0.791744  0.046100  0.149518  0.746968   \n",
       "4  0.302666 -0.425505 -0.191070 -0.414490 -0.031847 -0.008057 -0.537349   \n",
       "\n",
       "       7         8         9     ...      1490      1491      1492      1493  \\\n",
       "0 -0.146260  0.037089 -0.039497  ...  0.030712 -0.003086 -0.059155 -0.000657   \n",
       "1 -0.184241  0.143812 -0.045845  ... -0.028753  0.066659  0.047388 -0.090217   \n",
       "2 -0.259475 -0.463401 -0.286365  ...  0.002015 -0.030375 -0.038482 -0.039791   \n",
       "3  0.231181 -0.740373 -0.375252  ...  0.027823 -0.038555  0.016616 -0.065417   \n",
       "4 -0.144615  0.060322 -0.075051  ... -0.034048  0.019152  0.009938  0.023361   \n",
       "\n",
       "       1494      1495      1496      1497      1498      1499  \n",
       "0 -0.011793  0.038035  0.014311 -0.028168  0.021815 -0.038385  \n",
       "1  0.017659  0.019931  0.030875 -0.018938 -0.024505  0.054116  \n",
       "2  0.000306 -0.017464 -0.033838  0.035390 -0.080861  0.048241  \n",
       "3  0.053753 -0.037554 -0.006230  0.032986  0.054645  0.027326  \n",
       "4 -0.008969  0.014082  0.004835 -0.064524  0.039825  0.011461  \n",
       "\n",
       "[5 rows x 1500 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "dim = len(tdm_pca.columns)\n",
    "num_elements = len(tdm_pca)\n",
    "p = hnswlib.Index(space = 'cosine', dim = dim)\n",
    "\n",
    "p.init_index(max_elements = num_elements, ef_construction = 200, M = 16)\n",
    "p.add_items(tdm_pca)\n",
    "p.set_ef(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    a = to_tdm.transform([query]).toarray()\n",
    "    labels, distances = p.knn_query(pca.transform(a), k = 10)\n",
    "    return [i for i in verif_df.claim.iloc[labels[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Academy Awards have 24 awards.', 'The 79th Academy Awards honored films.', 'All About Eve won 6 Academy Awards.', 'Five Academy Awards were won by Braveheart.', 'The Academy Awards are given annually.', 'The Academy Awards are overseen by AMPAS.', 'The Incredibles won two Academy Awards.', \"Schindler's List received seven Academy Awards.\", 'The 79th Academy Awards began at 5:30 p.m. PST / 8:30 p.m. EST.', 'Braveheart was nominated for ten Academy Awards.']\n"
     ]
    }
   ],
   "source": [
    "print(process_query(input()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IncerementalPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IncerementalPCA takes an hour on google colab to train. It is not the worst time considering other data science tasks, however it is still too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [09:41<00:00, 581.57s/it]0.8526924910034374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import  IncrementalPCA\n",
    "import numpy as np\n",
    "import tqdm\n",
    "ipca =  IncrementalPCA(n_components=1500)\n",
    "num_rows= len(verif_df)//8\n",
    "chunk_size = 10000\n",
    "for i in tqdm.tqdm(range(0, num_rows//chunk_size)):\n",
    "    X = to_tdm.transform(verif_df[i*chunk_size : (i+1)*chunk_size]['claim'])\n",
    "    ipca.partial_fit(pd.DataFrame(X.toarray(), columns=to_tdm.get_feature_names()))\n",
    "print(sum(ipca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8526924910034374\n"
     ]
    }
   ],
   "source": [
    "X = to_tdm.transform(verif_df['claim'][:len(verif_df)//8])\n",
    "TDM = pd.DataFrame(X.toarray(), columns=to_tdm.get_feature_names())\n",
    "tdm_ipca = pd.DataFrame(ipca.transform(TDM))\n",
    "print(sum(ipca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "dim = len(tdm_ipca.columns)\n",
    "num_elements = len(tdm_ipca)\n",
    "p = hnswlib.Index(space = 'cosine', dim = dim)\n",
    "\n",
    "p.init_index(max_elements = num_elements, ef_construction = 200, M = 16)\n",
    "p.add_items(tdm_ipca)\n",
    "p.set_ef(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Academy Awards have 24 awards.', 'All About Eve won 6 Academy Awards.', 'Five Academy Awards were won by Braveheart.', 'The 84th Academy Awards winners included Rango.', 'The Academy Awards are an annual event.', 'Jerry Goldsmith was nominated for eighteen Academy Awards.', 'Winona Rider was nominated for two Academy Awards.', 'The 84th Academy Awards winners included Saving Face.', 'Tom Cruise has been nominated for Academy Awards.', 'Judd Apatow has been nominated for Academy Awards.']\n"
     ]
    }
   ],
   "source": [
    "def process_query(query):\n",
    "    a = to_tdm.transform([query]).toarray()\n",
    "    labels, distances = p.knn_query(ipca.transform(a), k = 10)\n",
    "    return [i for i in verif_df.claim.iloc[labels[0]]]\n",
    "print(process_query(input()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec\n",
    "\n",
    "Results are mediocre and training time is not that good as well. Maybe, using pretrained model improves the quality of the search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(nltk.word_tokenize(doc.lower()), [doc]) for i, doc in verif_df.claim.iteritems()]\n",
    "model = Doc2Vec(\n",
    "    documents,\n",
    "    vector_size=1000,\n",
    "    window=2,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=25,\n",
    "    # hs=1,\n",
    "    negative=5\n",
    ")\n",
    "\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is a movie called The Hunger Games.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[8].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.015170</td>\n",
       "      <td>0.009911</td>\n",
       "      <td>-0.005649</td>\n",
       "      <td>0.044883</td>\n",
       "      <td>-0.070835</td>\n",
       "      <td>0.018299</td>\n",
       "      <td>0.079309</td>\n",
       "      <td>-0.032559</td>\n",
       "      <td>0.017582</td>\n",
       "      <td>-0.022468</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039067</td>\n",
       "      <td>0.009005</td>\n",
       "      <td>0.029349</td>\n",
       "      <td>0.010545</td>\n",
       "      <td>-0.081545</td>\n",
       "      <td>0.012770</td>\n",
       "      <td>0.030488</td>\n",
       "      <td>0.003625</td>\n",
       "      <td>0.046975</td>\n",
       "      <td>-0.043632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.017702</td>\n",
       "      <td>-0.056137</td>\n",
       "      <td>-0.032280</td>\n",
       "      <td>0.095168</td>\n",
       "      <td>-0.030644</td>\n",
       "      <td>-0.090850</td>\n",
       "      <td>-0.076155</td>\n",
       "      <td>-0.003721</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>-0.092783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>0.015115</td>\n",
       "      <td>-0.026281</td>\n",
       "      <td>0.058902</td>\n",
       "      <td>0.039290</td>\n",
       "      <td>-0.040519</td>\n",
       "      <td>0.103330</td>\n",
       "      <td>-0.070613</td>\n",
       "      <td>-0.033845</td>\n",
       "      <td>-0.023018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.192599</td>\n",
       "      <td>-0.288584</td>\n",
       "      <td>-0.088721</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>-0.106758</td>\n",
       "      <td>0.072062</td>\n",
       "      <td>0.091159</td>\n",
       "      <td>0.283151</td>\n",
       "      <td>-0.086075</td>\n",
       "      <td>-0.105104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173682</td>\n",
       "      <td>-0.184560</td>\n",
       "      <td>0.126438</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>-0.006492</td>\n",
       "      <td>-0.141945</td>\n",
       "      <td>0.186002</td>\n",
       "      <td>-0.068589</td>\n",
       "      <td>0.081722</td>\n",
       "      <td>0.049389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.066397</td>\n",
       "      <td>0.102317</td>\n",
       "      <td>-0.013497</td>\n",
       "      <td>0.010727</td>\n",
       "      <td>0.055052</td>\n",
       "      <td>-0.089071</td>\n",
       "      <td>-0.047674</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.007775</td>\n",
       "      <td>-0.116966</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073393</td>\n",
       "      <td>0.005211</td>\n",
       "      <td>-0.044350</td>\n",
       "      <td>0.036146</td>\n",
       "      <td>0.056494</td>\n",
       "      <td>-0.010376</td>\n",
       "      <td>0.024438</td>\n",
       "      <td>-0.063543</td>\n",
       "      <td>-0.046629</td>\n",
       "      <td>0.033080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.105879</td>\n",
       "      <td>-0.089824</td>\n",
       "      <td>-0.032577</td>\n",
       "      <td>0.059719</td>\n",
       "      <td>0.046317</td>\n",
       "      <td>0.020823</td>\n",
       "      <td>0.119760</td>\n",
       "      <td>0.156948</td>\n",
       "      <td>0.031081</td>\n",
       "      <td>-0.073652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081380</td>\n",
       "      <td>-0.024795</td>\n",
       "      <td>0.083456</td>\n",
       "      <td>-0.093101</td>\n",
       "      <td>-0.071669</td>\n",
       "      <td>0.058142</td>\n",
       "      <td>-0.017791</td>\n",
       "      <td>-0.069858</td>\n",
       "      <td>0.030407</td>\n",
       "      <td>0.061695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.015170  0.009911 -0.005649  0.044883 -0.070835  0.018299  0.079309   \n",
       "1 -0.017702 -0.056137 -0.032280  0.095168 -0.030644 -0.090850 -0.076155   \n",
       "2  0.192599 -0.288584 -0.088721  0.032468 -0.106758  0.072062  0.091159   \n",
       "3  0.066397  0.102317 -0.013497  0.010727  0.055052 -0.089071 -0.047674   \n",
       "4  0.105879 -0.089824 -0.032577  0.059719  0.046317  0.020823  0.119760   \n",
       "\n",
       "        7         8         9    ...       240       241       242       243  \\\n",
       "0 -0.032559  0.017582 -0.022468  ... -0.039067  0.009005  0.029349  0.010545   \n",
       "1 -0.003721 -0.000136 -0.092783  ...  0.003352  0.015115 -0.026281  0.058902   \n",
       "2  0.283151 -0.086075 -0.105104  ...  0.173682 -0.184560  0.126438  0.004150   \n",
       "3  0.021300  0.007775 -0.116966  ... -0.073393  0.005211 -0.044350  0.036146   \n",
       "4  0.156948  0.031081 -0.073652  ...  0.081380 -0.024795  0.083456 -0.093101   \n",
       "\n",
       "        244       245       246       247       248       249  \n",
       "0 -0.081545  0.012770  0.030488  0.003625  0.046975 -0.043632  \n",
       "1  0.039290 -0.040519  0.103330 -0.070613 -0.033845 -0.023018  \n",
       "2 -0.006492 -0.141945  0.186002 -0.068589  0.081722  0.049389  \n",
       "3  0.056494 -0.010376  0.024438 -0.063543 -0.046629  0.033080  \n",
       "4 -0.071669  0.058142 -0.017791 -0.069858  0.030407  0.061695  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_df = pd.DataFrame([model.infer_vector(nltk.word_tokenize(doc.lower())) for doc in verif_df.claim])\n",
    "d2v_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "dim = len(d2v_df.columns)\n",
    "num_elements = len(d2v_df)\n",
    "p = hnswlib.Index(space = 'cosine', dim = dim)\n",
    "\n",
    "p.init_index(max_elements = num_elements, ef_construction = 200, M = 16)\n",
    "p.add_items(d2v_df)\n",
    "p.set_ef(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_d2v(query):\n",
    "    a = model.infer_vector(nltk.word_tokenize(query.lower()))\n",
    "    labels, distances = p.knn_query(a, k = 10)\n",
    "    return [i for i in verif_df.claim.iloc[labels[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8431 43566 34240 51507 29183 73858 34000 25407 27924 17363]]\n",
      "['Sarah Paulson was nominated for a Golden Globe Award.', 'Shania Twain plays music.', 'John Mayer won a Grammy Award.', 'Bob Marley wrote songs.', 'Anne Bancroft won two Emmy Awards.', 'Claire Danes received a Golden Globe Award.', 'John Cena is a professional WWE wrestler.', 'The 84th Academy Awards winners included Undefeated.', 'Foxcatcher was nominated for Best Actor.', 'Rihanna has sold more than 230 million records worldwide.']\n"
     ]
    }
   ],
   "source": [
    "print(process_query_d2v('Academy Awards'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Winona Ryder was nominated for an Academy Award.', 0.9120476245880127), ('Walt Disney smoked.', 0.9055764079093933), ('Liverpool F.C. plays football.', 0.9034287333488464), ('Guyana shares a border with Venezuela.', 0.9022931456565857), (\"Inhumans's main character's full name is Blackagar Boltagon.\", 0.9004422426223755), ('Scotland includes islands.', 0.8989565372467041), ('Maggie Gyllenhaal acts.', 0.8986670970916748), ('Daniel Craig has appeared in multiple movies.', 0.898438036441803), ('Doctor Strange is a fictional superhero.', 0.8980916738510132), ('Selene serves as a character.', 0.8965005874633789)]\n"
     ]
    }
   ],
   "source": [
    "query = 'Academy Award.'\n",
    "print(model.docvecs.most_similar(positive=[model.infer_vector(nltk.word_tokenize(query.lower()))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model\n",
    "\n",
    "I used roberta-base model from huggingface. It gives results that are somewhat similar to doc2vec results and take too long to encode vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f.query.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTextRepresentation were not initialized from the model checkpoint at roberta-base and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.language_representation import RepresentationModel\n",
    "\n",
    "model = RepresentationModel(model_type='roberta', model_name='roberta-base', use_cuda=True)\n",
    "\n",
    "vec = model.encode_sentences(verif_df[:20000].claim, combine_strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df = pd.DataFrame(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "dim = len(vec_df.columns)\n",
    "num_elements = len(vec_df)\n",
    "p = hnswlib.Index(space = 'cosine', dim = dim)\n",
    "\n",
    "p.init_index(max_elements = num_elements, ef_construction = 200, M = 16)\n",
    "p.add_items(vec_df)\n",
    "p.set_ef(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Frank Sinatra received critical acclaim for his performance in The Manchurian Candidate.', 'A co-producer of From the Earth to the Moon is credited to be Michael Bostick.', 'Aishwarya Raj was nominated eleven times for films Aishwarya Raj was in.', 'War Dogs stars Jonah Hill, Miles Teller, Ana de Armas and Bradley Cooper.', 'A 2009 war film featured Mike Myers starring in a small role.', 'The Attitude Era of WWE saw Dwayne \"The Rock\" Johnson become a major figure.', 'The programmer of Tetris was Alexey Pajitnov.', 'Maggie Gyllenhaal has been nominated for an Oscar.', \"Tesla Model S was ranked the world's best-selling plug-in electric car for 2015.\", 'Tyrese Gibson is well known for his role as Joseph \"Jody\" Summers in Baby Boy.']\n"
     ]
    }
   ],
   "source": [
    "def process_query(query):\n",
    "    a = model.encode_sentences(query, combine_strategy='mean')[0]\n",
    "    labels, distances = p.knn_query(a, k = 10)\n",
    "    return [i for i in verif_df.claim.iloc[labels[0]]]\n",
    "print(process_query('Academy Awards'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA fitted on subset, transform all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = to_tdm.transform(verif_df['claim'])\n",
    "TDM = pd.DataFrame(X.toarray(), columns=to_tdm.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8538933408821134\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1500)\n",
    "pca.fit(TDM.sample(n=10000))\n",
    "tdm_pca = pd.DataFrame(pca.transform(TDM))\n",
    "print(sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "dim = len(tdm_pca.columns)\n",
    "num_elements = len(tdm_pca)\n",
    "p = hnswlib.Index(space = 'cosine', dim = dim)\n",
    "\n",
    "p.init_index(max_elements = num_elements, ef_construction = 200, M = 16)\n",
    "p.add_items(tdm_pca)\n",
    "p.set_ef(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Braveheart won five Academy Awards.', 'Daniel Day-Lewis earned numerous awards like Academy Awards.', 'The Academy Awards have 24 awards.', 'Judi Dench has won Academy Awards.', 'The Academy Awards have multiple awards.', 'The 79th Academy Awards honored films.', 'Overwatch lets players gain cosmetic awards.', 'Jack Nicholson has won Academy Awards.', 'Laurence Olivier received four Academy Awards.', 'All About Eve won 6 Academy Awards.']\n"
     ]
    }
   ],
   "source": [
    "def process_query(query):\n",
    "    a = to_tdm.transform([query]).toarray()\n",
    "    labels, distances = p.knn_query(pca.transform(a), k = 10)\n",
    "    return [i for i in verif_df.claim.iloc[labels[0]]]\n",
    "print(process_query(input()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
